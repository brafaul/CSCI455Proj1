Date: Thu, 07 Nov 1996 19:07:56 GMT
Server: NCSA/1.5
Content-type: text/html
Last-modified: Mon, 02 Sep 1996 15:47:54 GMT
Content-length: 9489



 David Wood's Home Page 



 David Wood  (david@cs.wisc.edu) 




Associate Professor of Computer Science
and Electrical and Computer Engineering
Department of Computer Sciences
University of Wisconsin - Madison
1210 West Dayton Street
Madison, WI 53706  USA
david@cs.wisc.edu
Phone: 608-263-7463
Secretary: 265-4892 (Julie Fingerson or Thea Sklenar)
Departmental Office: 262-1204
Fax:  608-262-9777

Research Interests:

 
Computer architecture, 
especially memory system design for uniprocessors and multiprocessors.
 Design, implementation,  and programming of parallel computers.
 Operating systems for parallel computers.
 Performance evaluation tools and techniques, especially 
for memory system analysis.
 VLSI design, including low power design for portable computers.


Research Projects:

 Wisconsin Wind Tunnel (WWT)
 Memory System Performance Tools (WARTS)


Education:

 Ph.D. University of California, Berkeley, 1990
 B.S.  University of California, Berkeley, 1981


Current Graduate Students:

 Babak Falsafi
 Steve Reinhardt
 Brian Toonen


Recently Graduated Students:

 Rahmat Hyder (Intel)
 Alvy Lebeck (Duke University)
 Rob Pfile (Sun Microsystems)
 Mark Callaghan (Informix)


Courses I Teach:

Fall 1996: 

CS/ECE 552 - Introduction to Computer Architecture



CS/ECE 354 - Machine Organization and Programming


CS/ECE 552 - Introduction to Computer Architecture


CS/ECE 752 - Advanced Computer Architecture I


CS/ECE 757 - Advanced Computer Architecture II




 Selected Recent Papers 




Decoupled Hardware Support for Distributed Shared Memory


Steven K. Reinhardt, Robert W. Pfile, and
David A. Wood,
ACM/IEEE International Symposium on Computer Architecture (ISCA), 
May 1996




Coherent Network Interfaces for Fine-Grain Communication


Shubhendu S. Mukherjee and Babak Falsafi and Mark D. Hill and
David A. Wood,
ACM/IEEE International Symposium on Computer Architecture (ISCA), 
May 1996




Synchronization Hardware for Networks of Workstations: Performance vs. Cost


Rahmat S. Hyder and David A. Wood,
ACM/IEEE International Conference on Supercomputing (ICS), 
May 1996




Dynamic Self-Invalidation: Reducing Coherence Overhead in Shared-Memory Multiprocessors


Alvin R. Lebeck and
David A. Wood,
ACM/IEEE International Symposium on Computer Architecture (ISCA), 
June 1995




Active Memory: A New Abstraction For Memory System Simulation


Alvin R. Lebeck and
David A. Wood,
ACM SIGMETRICS
May 1995



Accuracy vs. Performance in Parallel Simulation of Interconnection Networks,
Douglas C. Burger and David A. Wood.
In the proceedings of the 9th International Parallel Processing Symposium, April, 1995.



Application-Specific Protocols for User-Level Shared Memory,


Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas,
Mark Hill, James Larus, Anne Rogers, and David Wood,
In Proceedings of Supercomputing '94.




Fine-grain Access Control for Distributed Shared Memory,


Ioannis Schoinas, Babak Falsafi, Alvin Lebeck, Steven Reinhardt,
James Larus, and David Wood,
Proceedings of ASPLOS VI.




Tempest and Typhoon: User-Level Shared Memory,


Steven Reinhardt, James Larus, and David Wood,
Proceedings of Int'l Symposium on Computer Architecture, 1994.




Cache Profiling and the SPEC Benchmarks: A Case Study,


Alvin R. Lebeck and
David A. Wood,
pages 15-26,
IEEE COMPUTER,
October 1994




Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors,


Mark D. Hill, James R. Larus, Steven K. Reinhardt, David A. Wood,
ACM Transactions on Computer Systems (TOCS), November 1993.




The Wisconsin Wind Tunnel Project: An Annotated Bibliography,


Mark D. Hill, James R. Larus, David A. Wood,
Computer Architecture News, v. 22, n. 5, December 1994.
On-line version revised frequently.




Wisconsin Architectural Research Tool Set (WARTS),


Mark D. Hill, James R. Larus, Alvin R. Lebeck, Madhusudhan Talluri,
David A. Wood,
Computer Architecture News (CAN), August 1993.



 Research Summary  

My main research goals lie in developing cost-effective computer
architectures that take advantage of rapidly changing technologies.  My
research program has two major thrusts:  

 evaluating the performance,
feasibility, and correctness of new architectures, and
 developing new tools and techniques to facilitate this evaluation.

Currently, this research focusses on the following three areas:

 multi-paradigm multiprocessors,
which efficiently integrate shared-memory, message-passing, and hybrid
programming paradigms,
 a virtual prototyping system, which exploits the similarites
of an existing parallel machine to simulate a hypothetical parallel machine,
 and, techniques for understanding and tuning program performance.


Recent results include developing a new interface---called
Tempest---between user-level protocol handlers and system-supplied
mechanisms. Tempest provides the mechanisms that allow programmers,
compilers, and program libraries to implement and use message passing,
transparent shared memory, and hybrid combinations of the two.  Tempest
mechanisms are low-overhead messages, bulk data transfer, virtual
memory management, and fine-grain access control.  The most novel
mechanism---fine-grain access control---allows user software to tag
blocks (e.g., 32 bytes) as read-write, read-only, or invalid, so the
local memory can be used to transparently cache remote data.

We are exploring alternative ways to support this interface.
The first---called Typhoon---is
a proposed hardware
platform that implements the Tempest mechanisms with a fully-programmable,
user-level processor in the network interface.  A reverse-translation
table (RTLB) invokes the network processor when it detects a fine-grain
access fault.
We have simulated Typhoon on the Wisconsin Wind Tunnel and found that
a transparent shared-memory protocol running on Typhoon performs
comparably +/- 30% to an
all-hardware Dir{N}NB cache-coherence protocol for five shared-memory
programs.

We have also developed a new memory system simulation method that
optimizes the common case---cache hits---significantly reducing
simulation time.
Fast-Cache tightly integrates reference generation and simulation by
providing the abstraction of tagged memory blocks: each reference
invokes a user-specified function depending upon the reference type and
memory block state. The simulator controls how references are processed
by manipulating memory block states, specifying a special NULL function
for no action cases.  Fast-Cache implements this abstraction by using
binary-rewriting to perform a table lookup before each memory
reference. On a SPARCStation 10, Fast-Cache simulation times are two to
three times faster than a conventional trace-driven simulator that
calls a procedure on each memory reference; simulation times are only
three to six times slower than the original, un-instrumented program.
We are also investigating using Fast-Cache's binary rewriting techniques
to support the Tempest interface on existing hardware platforms.


 Last Updated: July 11, 1996 



